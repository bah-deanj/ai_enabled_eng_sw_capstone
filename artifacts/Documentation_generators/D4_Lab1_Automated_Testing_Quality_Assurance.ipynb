{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4.1'\n",
      "Successfully imported modules and set up the LLM client.\n",
      "Successfully loaded c:\\Users\\labadmin\\Desktop\\capstone\\ai_enabled_eng_sw_capstone\\app\\main.py.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    # Assumes the notebook is in 'labs/Day_01_.../'\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    # Fallback for different execution environments\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from app.agent.utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "\n",
    "# You can now use the imported functions\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")\n",
    "\n",
    "print(\"Successfully imported modules and set up the LLM client.\")\n",
    "\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "# app_code = load_artifact(\"app/main.py\")\n",
    "# if not app_code:\n",
    "#     print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")\n",
    "\n",
    "# Uses absolute path to avoid path issues in different environments\n",
    "file_path = r\"c:\\Users\\labadmin\\Desktop\\capstone\\ai_enabled_eng_sw_capstone\\app\\main.py\"\n",
    "app_code = None\n",
    "try:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        app_code = f.read()\n",
    "except FileNotFoundError:\n",
    "    pass  # app_code remains None\n",
    "\n",
    "if not app_code:\n",
    "    print(f\"Warning: Could not load {file_path}. Lab may not function correctly.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_tests_prompt_base = f\"\"\"Provide a detailed prompt for creating happy path tests with python\n",
    "adding a placeholder for the context needed for effective prompt\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here’s a detailed prompt you can use (or give to an LLM) to generate **happy path tests in Python**. There’s a placeholder for you to add the **context** (such as a description of the function, code, or API to be tested):\n",
      "\n",
      "---\n",
      "\n",
      "**Prompt for Creating Happy Path Tests in Python**\n",
      "\n",
      "You are an expert Python software tester. Please write a set of **happy path unit tests** in Python for the following context:\n",
      "\n",
      "[INSERT CONTEXT HERE: e.g., function description, code snippet, API endpoint, class, etc.]\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "- Use the `unittest` or `pytest` framework (specify which one in your example).\n",
      "- Focus on **happy path scenarios**: valid inputs, expected outputs, and standard (non-error) execution flows.\n",
      "- For each test, explain what is being tested and why it’s a happy path.\n",
      "- Make sure your test names are descriptive.\n",
      "- If necessary, include any setup or teardown logic.\n",
      "- Assume all dependencies are available.\n",
      "\n",
      "**Example Context Placeholder:**\n",
      "```python\n",
      "# Example function to be tested\n",
      "def add(a, b):\n",
      "    \"\"\"Returns the sum of a and b.\"\"\"\n",
      "    return a + b\n",
      "```\n",
      "\n",
      "**Now, generate the happy path tests accordingly.**\n",
      "\n",
      "---\n",
      "\n",
      "**How to use:**  \n",
      "Replace `[INSERT CONTEXT HERE]` with your actual function, API, class, or code description.  \n",
      "For example, if you want to test a `login_user(username, password)` function, paste its code or describe its expected behavior in the placeholder.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you want a **filled-in example**!\n"
     ]
    }
   ],
   "source": [
    "happy_path_prompt = get_completion(happy_path_tests_prompt_base, client, model_name, api_provider)\n",
    "print(happy_path_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "# tests/test_main_simple_1.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from main import app  # Assuming your FastAPI app is defined in main.py\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "def test_create_user_successful():\n",
      "    \"\"\"\n",
      "    Test that a user is created successfully with valid data.\n",
      "    \"\"\"\n",
      "    user_data = {\n",
      "        \"username\": \"john_doe\",\n",
      "        \"email\": \"john@example.com\",\n",
      "        \"full_name\": \"John Doe\",\n",
      "        \"password\": \"securepassword123\"\n",
      "    }\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code in (200, 201)\n",
      "    data = response.json()\n",
      "    assert \"id\" in data\n",
      "    assert data[\"username\"] == user_data[\"username\"]\n",
      "    assert data[\"email\"] == user_data[\"email\"]\n",
      "    assert data[\"full_name\"] == user_data[\"full_name\"]\n",
      "\n",
      "def test_create_another_user_successful():\n",
      "    \"\"\"\n",
      "    Test that a second user can be created successfully.\n",
      "    \"\"\"\n",
      "    user_data = {\n",
      "        \"username\": \"jane_smith\",\n",
      "        \"email\": \"jane@example.com\",\n",
      "        \"full_name\": \"Jane Smith\",\n",
      "        \"password\": \"anothersecurepwd\"\n",
      "    }\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code in (200, 201)\n",
      "    data = response.json()\n",
      "    assert \"id\" in data\n",
      "    assert data[\"username\"] == user_data[\"username\"]\n",
      "    assert data[\"email\"] == user_data[\"email\"]\n",
      "    assert data[\"full_name\"] == user_data[\"full_name\"]\n",
      "\n",
      "def test_get_users_returns_created_users():\n",
      "    \"\"\"\n",
      "    Test that the GET /users/ endpoint returns a list including previously created users.\n",
      "    \"\"\"\n",
      "    # Create two users\n",
      "    user1 = {\n",
      "        \"username\": \"alice\",\n",
      "        \"email\": \"alice@example.com\",\n",
      "        \"full_name\": \"Alice Wonderland\",\n",
      "        \"password\": \"alicepwd\"\n",
      "    }\n",
      "    user2 = {\n",
      "        \"username\": \"bob\",\n",
      "        \"email\": \"bob@example.com\",\n",
      "        \"full_name\": \"Bob Builder\",\n",
      "        \"password\": \"bobpwd\"\n",
      "    }\n",
      "    client.post(\"/users/\", json=user1)\n",
      "    client.post(\"/users/\", json=user2)\n",
      "\n",
      "    response = client.get(\"/users/\")\n",
      "    assert response.status_code == 200\n",
      "    users = response.json()\n",
      "    assert isinstance(users, list)\n",
      "    usernames = [user[\"username\"] for user in users]\n",
      "    assert user1[\"username\"] in usernames\n",
      "    assert user2[\"username\"] in usernames\n",
      "✅ Successfully saved artifact to: artifacts\\tests\\test_main_simple_1.py\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are an expert Python developer and test engineer.  \n",
    "Given the following context about the code/functionality to be tested, generate clear, self-contained happy path test cases\n",
    "using Python and the `pytest` framework fpr the `POST /users/` endpoint.\n",
    "Assert that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the \n",
    "response body).\n",
    "Generate another test for the `GET /users/` endpoint.\n",
    "Save the generated tests into a file named `tests/test_main_simple_1.py`\n",
    "A happy path test checks that the code works as expected under normal, valid conditions (no errors or edge cases).\n",
    "\n",
    "**Instructions:**  \n",
    "- Write at least 3 happy path test cases covering typical valid scenarios.\n",
    "- Use clear and descriptive test function/method names.\n",
    "- Each test should be independently runnable.\n",
    "- Assume all necessary imports (import the function to be tested and the test framework).\n",
    "- Do not test error handling or edge cases unless they are part of the intended happy path.\n",
    "- Include any setup or test data needed.\n",
    "- Output only the code for the tests.\n",
    "\n",
    "Use the app_code below as context for generating the tests.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple_1.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_case_tests_prompt_base = f\"\"\"Provide a detailed prompt for creating edge case tests with python\n",
    "adding a placeholder for the context needed for an effective prompt\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is a detailed prompt template for generating edge case tests in Python, with a placeholder for you to add context about the function or system under test:\n",
      "\n",
      "---\n",
      "\n",
      "**Prompt Template for Edge Case Tests in Python**\n",
      "\n",
      "> **Context:**  \n",
      "> [Insert a clear and concise description of the function, method, or system under test. Include its purpose, expected inputs and outputs, constraints, and any relevant business rules or assumptions.]\n",
      "\n",
      "> **Instructions:**  \n",
      "> - Based on the above context, generate a comprehensive set of Python test cases focused on edge cases and boundary conditions.  \n",
      "> - For each test, include a brief comment explaining why it is considered an edge case.  \n",
      "> - Implement the tests using Python’s built-in `unittest` framework (or another testing library, if specified in the context).  \n",
      "> - Ensure the tests are clear, concise, and self-contained.  \n",
      "> - Cover all relevant input types, minimum and maximum values, empty inputs, invalid formats, and any other scenarios that might reveal bugs or unintended behavior.\n",
      "\n",
      "---\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "> **Context:**  \n",
      "> We have a function `def divide(a, b):` that returns the result of dividing `a` by `b`. It should raise a `ValueError` if `b` is zero.\n",
      "\n",
      "> **Instructions:**  \n",
      "> (as above)\n",
      "\n",
      "---\n",
      "\n",
      "**You can copy and adapt this template for your own edge case testing needs by filling in the [Context] placeholder with details about your specific function or system.**\n"
     ]
    }
   ],
   "source": [
    "edge_case_prompt = get_completion(edge_case_tests_prompt_base, client, model_name, api_provider)\n",
    "print(edge_case_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "# File: tests/test_main_edge_1.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "# Import your FastAPI app here\n",
      "# from app.main import app\n",
      "# For demonstration, we'll assume `app` is imported\n",
      "\n",
      "# Replace this import with your actual app\n",
      "from app import app\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "# =========================\n",
      "# POST /users/ edge case tests\n",
      "# =========================\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"user_data, expected_error\",\n",
      "    [\n",
      "        # Empty payload: No data at all\n",
      "        ({}, \"Missing required fields\"),\n",
      "        # Missing username\n",
      "        ({\"email\": \"test@example.com\", \"password\": \"password123\"}, \"username\"),\n",
      "        # Missing email\n",
      "        ({\"username\": \"testuser\", \"password\": \"password123\"}, \"email\"),\n",
      "        # Missing password\n",
      "        ({\"username\": \"testuser\", \"email\": \"test@example.com\"}, \"password\"),\n",
      "        # Empty username\n",
      "        ({\"username\": \"\", \"email\": \"test@example.com\", \"password\": \"password123\"}, \"username\"),\n",
      "        # Empty email\n",
      "        ({\"username\": \"testuser\", \"email\": \"\", \"password\": \"password123\"}, \"email\"),\n",
      "        # Empty password\n",
      "        ({\"username\": \"testuser\", \"email\": \"test@example.com\", \"password\": \"\"}, \"password\"),\n",
      "        # Invalid email format\n",
      "        ({\"username\": \"testuser\", \"email\": \"not-an-email\", \"password\": \"password123\"}, \"email\"),\n",
      "        # Extremely long username\n",
      "        ({\"username\": \"a\" * 256, \"email\": \"test@example.com\", \"password\": \"password123\"}, \"username\"),\n",
      "        # Extremely short password\n",
      "        ({\"username\": \"testuser\", \"email\": \"test@example.com\", \"password\": \"1\"}, \"password\"),\n",
      "        # Username with special characters (assuming disallowed)\n",
      "        ({\"username\": \"user!@#\", \"email\": \"test@example.com\", \"password\": \"password123\"}, \"username\"),\n",
      "        # Email with spaces\n",
      "        ({\"username\": \"testuser\", \"email\": \"test user@example.com\", \"password\": \"password123\"}, \"email\"),\n",
      "        # Password with only spaces\n",
      "        ({\"username\": \"testuser\", \"email\": \"test@example.com\", \"password\": \"   \"}, \"password\"),\n",
      "    ],\n",
      ")\n",
      "def test_post_users_invalid_edge_cases(user_data, expected_error):\n",
      "    \"\"\"\n",
      "    Edge case tests for POST /users/ with various invalid inputs.\n",
      "    Each test checks for 400 Bad Request and presence of the expected error message.\n",
      "    \"\"\"\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 400, f\"Expected 400 for input: {user_data}\"\n",
      "    # Response should contain the expected error in the JSON body\n",
      "    body = response.json()\n",
      "    error_message = body.get(\"error\") or body.get(\"detail\") or str(body)\n",
      "    assert expected_error.lower() in error_message.lower(), (\n",
      "        f\"Expected error mentioning '{expected_error}', got: {body}\"\n",
      "    )\n",
      "\n",
      "# =========================\n",
      "# GET /users/ edge case test\n",
      "# =========================\n",
      "\n",
      "def test_get_users_when_none_exist(monkeypatch):\n",
      "    \"\"\"\n",
      "    Edge case: GET /users/ when there are no users in the system.\n",
      "    The response should be 200 OK and return an empty list.\n",
      "    \"\"\"\n",
      "    # Monkeypatch the user fetching logic to simulate no users (if needed)\n",
      "    # For illustration, we assume the database is empty\n",
      "\n",
      "    response = client.get(\"/users/\")\n",
      "    assert response.status_code == 200\n",
      "    users = response.json()\n",
      "    assert isinstance(users, list), f\"Expected a list, got {type(users)}\"\n",
      "    assert len(users) == 0, f\"Expected empty list, got {users}\"\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are an expert Python developer and test engineer.\n",
    "Use the following context about the code/functionality to be tested, generate clear, self-contained edge case test cases\n",
    "using Python and the `pytest` framework for the `POST /users/` endpoint.  \n",
    "Assert that invalid user data is handled correctly (e.g., checking for a `400 Bad Request` status code and verifying the \n",
    "response body contains appropriate error messages).\n",
    "Generate another test for the `GET /users/` endpoint.\n",
    "Save the generated tests into a file named `tests/test_main_edge_1.py`\n",
    "An edge case test checks how the code handles unusual, extreme, or unexpected inputs or situations. \n",
    "\n",
    "Instructions:\n",
    "{edge_case_prompt}\n",
    "\n",
    "\n",
    "Use the app_code below as context for generating the tests.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\"\n",
    "AS a expert Python developer and test engineer.\n",
    "Using the context below about the code/functionality to be tested, generate a pytest fixture that sets up an isolated test database\n",
    "using SQLite in-memory database for testing purposes. The fixture should create the database schema before each test and \n",
    "tear it down afterward.\n",
    "Include any necessary imports and setup code.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest_1.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "As an expert Python developer and test engineer.\n",
    "Using the context below about the code/functionality to be tested, refactor the happy path tests to utilize the new pytest fixture for the isolated test database.\n",
    "Ensure that the tests are still clear, self-contained, and independently runnable.\n",
    "Include any necessary imports and setup code.\n",
    "Save the refactored tests into a file named `tests/test_main_with_fixture_1.py`\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture_1.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
